{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fra.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists for store the samples\n",
    "en_samples,de_samples = [],[]\n",
    "#define sets to store the characters in them\n",
    "en_chars, de_chars = set(), set()\n",
    "\n",
    "# Split the samples and get the character sets :\n",
    "for line in text:\n",
    "    en_ , de_ = line.split('\\t')\n",
    "    de_ = '\\t' + de_\n",
    "    for char in de_:\n",
    "        if char not in de_chars:\n",
    "            de_chars.add(char)\n",
    "    for char in en_:\n",
    "        if char not in en_chars:\n",
    "            en_chars.add(char)\n",
    "    en_samples.append(en_)\n",
    "    de_samples.append(de_)\n",
    "   \n",
    "# Add the chars \\t and \\n to the sets \n",
    "de_chars.add('\\n')\n",
    "de_chars.add('\\t')\n",
    "\n",
    "en_chars.add('\\n')\n",
    "en_chars.add('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167130\n",
      "Go.\n"
     ]
    }
   ],
   "source": [
    "print(len(en_samples))\n",
    "print(en_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167130\n",
      "\tVa !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(de_samples))\n",
    "print(de_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'–', '’', 'Q', '%', 't', 'I', 'v', '\\xad', '2', 'ö', 'X', 'u', 'A', '8', 'k', 'g', 'ç', ',', 'n', 'y', '3', '1', '9', 'K', 'V', \"'\", 'z', '\"', ';', '+', 'B', 'ú', 'p', 'm', '\\xa0', 'а', 'F', 'N', '5', 'T', 'h', '\\n', 's', '4', 'c', 'W', '€', 'L', 'f', 'Y', '₂', 'e', ':', 'o', 'j', 'd', 'M', 'D', '—', 'J', 'i', '/', 'S', '‘', '7', 'r', 'é', 'R', '$', 'q', 'Z', 'H', 'E', 'x', '?', 'a', 'b', 'C', 'O', '.', '\\t', 'G', '!', 'U', '-', '0', '6', 'P', 'º', '&', 'l', 'w', ' '}\n"
     ]
    }
   ],
   "source": [
    "print(en_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the needed dictionaries to convert characters to integers and vise versa.....\n",
    "de_char_to_int, de_int_to_char , en_char_to_int, en_int_to_char = dict(), dict(), dict(), dict()\n",
    "for i, char in enumerate(en_chars):\n",
    "    en_char_to_int[char] = i\n",
    "    en_int_to_char[i] = char\n",
    "for j, char in enumerate(de_chars):\n",
    "    de_char_to_int[char] = j\n",
    "    de_int_to_char[j] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'–': 0, '’': 1, 'Q': 2, '%': 3, 't': 4, 'I': 5, 'v': 6, '\\xad': 7, '2': 8, 'ö': 9, 'X': 10, 'u': 11, 'A': 12, '8': 13, 'k': 14, 'g': 15, 'ç': 16, ',': 17, 'n': 18, 'y': 19, '3': 20, '1': 21, '9': 22, 'K': 23, 'V': 24, \"'\": 25, 'z': 26, '\"': 27, ';': 28, '+': 29, 'B': 30, 'ú': 31, 'p': 32, 'm': 33, '\\xa0': 34, 'а': 35, 'F': 36, 'N': 37, '5': 38, 'T': 39, 'h': 40, '\\n': 41, 's': 42, '4': 43, 'c': 44, 'W': 45, '€': 46, 'L': 47, 'f': 48, 'Y': 49, '₂': 50, 'e': 51, ':': 52, 'o': 53, 'j': 54, 'd': 55, 'M': 56, 'D': 57, '—': 58, 'J': 59, 'i': 60, '/': 61, 'S': 62, '‘': 63, '7': 64, 'r': 65, 'é': 66, 'R': 67, '$': 68, 'q': 69, 'Z': 70, 'H': 71, 'E': 72, 'x': 73, '?': 74, 'a': 75, 'b': 76, 'C': 77, 'O': 78, '.': 79, '\\t': 80, 'G': 81, '!': 82, 'U': 83, '-': 84, '0': 85, '6': 86, 'P': 87, 'º': 88, '&': 89, 'l': 90, 'w': 91, ' ': 92}\n"
     ]
    }
   ],
   "source": [
    "print(en_char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '–', 1: '’', 2: 'Q', 3: '%', 4: 't', 5: 'I', 6: 'v', 7: '\\xad', 8: '2', 9: 'ö', 10: 'X', 11: 'u', 12: 'A', 13: '8', 14: 'k', 15: 'g', 16: 'ç', 17: ',', 18: 'n', 19: 'y', 20: '3', 21: '1', 22: '9', 23: 'K', 24: 'V', 25: \"'\", 26: 'z', 27: '\"', 28: ';', 29: '+', 30: 'B', 31: 'ú', 32: 'p', 33: 'm', 34: '\\xa0', 35: 'а', 36: 'F', 37: 'N', 38: '5', 39: 'T', 40: 'h', 41: '\\n', 42: 's', 43: '4', 44: 'c', 45: 'W', 46: '€', 47: 'L', 48: 'f', 49: 'Y', 50: '₂', 51: 'e', 52: ':', 53: 'o', 54: 'j', 55: 'd', 56: 'M', 57: 'D', 58: '—', 59: 'J', 60: 'i', 61: '/', 62: 'S', 63: '‘', 64: '7', 65: 'r', 66: 'é', 67: 'R', 68: '$', 69: 'q', 70: 'Z', 71: 'H', 72: 'E', 73: 'x', 74: '?', 75: 'a', 76: 'b', 77: 'C', 78: 'O', 79: '.', 80: '\\t', 81: 'G', 82: '!', 83: 'U', 84: '-', 85: '0', 86: '6', 87: 'P', 88: 'º', 89: '&', 90: 'l', 91: 'w', 92: ' '}\n"
     ]
    }
   ],
   "source": [
    "print(en_int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prabha Number of Encoder samples \t : 167130\n",
      "Prabha Number of Decoder samples \t : 167130\n",
      "Number of Encoder chars \t : 93\n",
      "Number of Decoder chars \t : 115\n",
      "The Longest Decoder Sample has 351 Chars\n",
      "The Longest Encoder Sample has 286 Chars\n"
     ]
    }
   ],
   "source": [
    "# getting lenghts and sizes of dataset\n",
    "num_en_chars = len(en_chars)\n",
    "num_de_chars = len(de_chars)\n",
    "\n",
    "max_en_chars_per_sample = max([len(sample) for sample in en_samples])\n",
    "max_de_chars_per_sample = max([len(sample) for sample in de_samples])\n",
    "\n",
    "num_en_samples = len(en_samples)\n",
    "num_de_samples = len(de_samples)\n",
    "\n",
    "print(f'Prabha Number of Encoder samples \\t : {num_en_samples}')\n",
    "print(f'Prabha Number of Decoder samples \\t : {num_de_samples}')\n",
    "\n",
    "print(f'Number of Encoder chars \\t : {len(en_chars)}')\n",
    "print(f'Number of Decoder chars \\t : {len(de_chars)}')\n",
    "\n",
    "print(f'The Longest Decoder Sample has {max_de_chars_per_sample} Chars')\n",
    "print(f'The Longest Encoder Sample has {max_en_chars_per_sample} Chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing numpy arrays to store the data to give to the seq2seq model:\n",
    "# Here we are creating numpy array with maximum encoder samples, maximum chars in all samples and num of unique chars\n",
    "encoder_input_data = np.zeros((num_en_samples,\n",
    "                               max_en_chars_per_sample,\n",
    "                               num_en_chars),\n",
    "                             dtype = 'float32')\n",
    "decoder_input_data = np.zeros((num_de_samples,\n",
    "                               max_de_chars_per_sample,\n",
    "                               num_de_chars),\n",
    "                             dtype = 'float32')\n",
    "target_data = np.zeros((num_de_samples,\n",
    "                        max_de_chars_per_sample,\n",
    "                        num_de_chars),\n",
    "                      dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is jump into one hot representation\n",
    "'''  here we represent each sample as an array of zeros that has (n) rows and (m) columns \n",
    "    where N -> Number of character that long word has\n",
    "           M -> Number of characters that dictionary has\n",
    "'''\n",
    "# we are making embeddings for three types of data 1. Encoder Input dat, 2. Decoder Input data and 3. Target Data\n",
    "# Decoder input = \"\\tHow are you\" and Targe = \"How are you\", \\t is removed there\n",
    "for index, (en_sample,de_sample) in enumerate(zip(en_samples, de_samples)):\n",
    "    for char, en_char in enumerate(en_sample):\n",
    "        encoder_input_data[index, char, en_char_to_int[en_char]] = 1\n",
    "    for char, de_char in enumerate(de_sample):\n",
    "        decoder_input_data[index, char, de_char_to_int[en_char]] = 1\n",
    "        if char > 0:\n",
    "            target_data[index, char-1, de_char_to_int[en_char]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Encoder Input Data   : (167130, 286, 93)\n",
      "Shape of Decoder Input Data   : (167130, 351, 115)\n",
      "Shape of Target Data          : (167130, 351, 115)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of Encoder Input Data   : {encoder_input_data.shape}')\n",
    "print(f'Shape of Decoder Input Data   : {decoder_input_data.shape}')\n",
    "print(f'Shape of Target Data          : {target_data.shape}')\n",
    "\n",
    "# Now the data is ready to be used by a Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# this is time for word processing(Using word embeddings)\n",
    "file_name = 'fra.txt'\n",
    "lines = pd.read_table(file_name, names=['en', 'de'], encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       en  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Hi.   \n",
      "2                                                    Run!   \n",
      "3                                                    Run!   \n",
      "4                                                    Who?   \n",
      "5                                                    Wow!   \n",
      "6                                                   Fire!   \n",
      "7                                                   Help!   \n",
      "8                                                   Jump.   \n",
      "9                                                   Stop!   \n",
      "10                                                  Stop!   \n",
      "11                                                  Stop!   \n",
      "12                                                  Wait!   \n",
      "13                                                  Wait!   \n",
      "14                                                 Go on.   \n",
      "15                                                 Go on.   \n",
      "16                                                 Go on.   \n",
      "17                                                 Hello!   \n",
      "18                                                 Hello!   \n",
      "19                                                 I see.   \n",
      "20                                                 I try.   \n",
      "21                                                 I won!   \n",
      "22                                                 I won!   \n",
      "23                                                 I won.   \n",
      "24                                                 Oh no!   \n",
      "25                                                Attack!   \n",
      "26                                                Attack!   \n",
      "27                                                Cheers!   \n",
      "28                                                Cheers!   \n",
      "29                                                Cheers!   \n",
      "...                                                   ...   \n",
      "167100  A man who has never gone to school may steal f...   \n",
      "167101  One way to lower the number of errors in the T...   \n",
      "167102  What is old age? First you forget names, then ...   \n",
      "167103  What is old age? First you forget names, then ...   \n",
      "167104  He and I have a near-telepathic understanding ...   \n",
      "167105  Although rainforests make up only two percent ...   \n",
      "167106  She has a boyfriend she's been going out with ...   \n",
      "167107  If you translate from your second language int...   \n",
      "167108  I love trying out new things, so I always buy ...   \n",
      "167109  A good theory is characterized by the fact tha...   \n",
      "167110  The more time you spend speaking a foreign lan...   \n",
      "167111  The enquiry concluded that, despite his denial...   \n",
      "167112  The Tatoeba Project, which can be found online...   \n",
      "167113  You may not learn to speak as well as a native...   \n",
      "167114  And the good news is that today the economy is...   \n",
      "167115  E-cigarettes are being promoted as a healthy a...   \n",
      "167116  It's still too hard to find a job. And even if...   \n",
      "167117  As you contribute more sentences to the Tatoeb...   \n",
      "167118  Even at the end of the nineteenth century, sai...   \n",
      "167119  Five tremors in excess of magnitude 5.0 on the...   \n",
      "167120  No matter how much you try to convince people ...   \n",
      "167121  We need to uphold laws against discrimination ...   \n",
      "167122  A child who is a native speaker usually knows ...   \n",
      "167123  There are four main causes of alcohol-related ...   \n",
      "167124  Top-down economics never works, said Obama. \"T...   \n",
      "167125  A carbon footprint is the amount of carbon dio...   \n",
      "167126  Death is something that we're often discourage...   \n",
      "167127  Since there are usually multiple websites on a...   \n",
      "167128  If someone who doesn't know your background sa...   \n",
      "167129  It may be impossible to get a completely error...   \n",
      "\n",
      "                                                       de  \n",
      "0                                                    Va !  \n",
      "1                                                 Salut !  \n",
      "2                                                 Cours !  \n",
      "3                                                Courez !  \n",
      "4                                                   Qui ?  \n",
      "5                                              Ça alors !  \n",
      "6                                                Au feu !  \n",
      "7                                              À l'aide !  \n",
      "8                                                  Saute.  \n",
      "9                                             Ça suffit !  \n",
      "10                                                 Stop !  \n",
      "11                                           Arrête-toi !  \n",
      "12                                              Attends !  \n",
      "13                                             Attendez !  \n",
      "14                                              Poursuis.  \n",
      "15                                             Continuez.  \n",
      "16                                            Poursuivez.  \n",
      "17                                              Bonjour !  \n",
      "18                                                Salut !  \n",
      "19                                          Je comprends.  \n",
      "20                                              J'essaye.  \n",
      "21                                           J'ai gagné !  \n",
      "22                                      Je l'ai emporté !  \n",
      "23                                            J’ai gagné.  \n",
      "24                                               Oh non !  \n",
      "25                                              Attaque !  \n",
      "26                                             Attaquez !  \n",
      "27                                                Santé !  \n",
      "28                                        À votre santé !  \n",
      "29                                                Merci !  \n",
      "...                                                   ...  \n",
      "167100  Un homme qui n'a jamais été à l'école peut vol...  \n",
      "167101  Un moyen de diminuer le nombre d’erreurs dans ...  \n",
      "167102  Qu'est l'âge ? D'abord on oublie les noms, et ...  \n",
      "167103  Ce qu'est l'âge ? D'abord on oublie les noms, ...  \n",
      "167104  Lui et moi avons une compréhension quasi-télép...  \n",
      "167105  Bien que les forêts tropicales ne couvrent que...  \n",
      "167106  Elle a un copain avec qui elle sort depuis le ...  \n",
      "167107  Si vous traduisez de votre seconde langue dans...  \n",
      "167108  J'adore essayer de nouvelles choses, alors j'a...  \n",
      "167109  Une bonne théorie se caractérise par le fait d...  \n",
      "167110  Plus l'on passe de temps à parler une langue é...  \n",
      "167111  L'enquête conclut qu'en dépit de ses dénégatio...  \n",
      "167112  Le projet Tatoeba, que l'on peut trouver en li...  \n",
      "167113  Peut-être n'apprendrez-vous pas à parler comme...  \n",
      "167114  Et la bonne nouvelle est qu'aujourd'hui l'écon...  \n",
      "167115  La cigarette électronique est mise en avant co...  \n",
      "167116  C'est encore trop difficile de trouver un empl...  \n",
      "167117  Au fur et à mesure que vous ajoutez davantage ...  \n",
      "167118  Même à la fin du dix-neuvième siècle, les mari...  \n",
      "167119  Cinq secousses dépassant la magnitude cinq sur...  \n",
      "167120  Peu importe le temps que tu passeras à essayer...  \n",
      "167121  Nous devons faire respecter les lois contre la...  \n",
      "167122  Un enfant qui est un locuteur natif connaît ha...  \n",
      "167123  Il y a quatre causes principales de décès liés...  \n",
      "167124  « L'économie en partant du haut vers le bas, ç...  \n",
      "167125  Une empreinte carbone est la somme de pollutio...  \n",
      "167126  La mort est une chose qu'on nous décourage sou...  \n",
      "167127  Puisqu'il y a de multiples sites web sur chaqu...  \n",
      "167128  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
      "167129  Il est peut-être impossible d'obtenir un Corpu...  \n",
      "\n",
      "[167130 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                       Go.\n",
      "1                                                       Hi.\n",
      "2                                                      Run!\n",
      "3                                                      Run!\n",
      "4                                                      Who?\n",
      "5                                                      Wow!\n",
      "6                                                     Fire!\n",
      "7                                                     Help!\n",
      "8                                                     Jump.\n",
      "9                                                     Stop!\n",
      "10                                                    Stop!\n",
      "11                                                    Stop!\n",
      "12                                                    Wait!\n",
      "13                                                    Wait!\n",
      "14                                                   Go on.\n",
      "15                                                   Go on.\n",
      "16                                                   Go on.\n",
      "17                                                   Hello!\n",
      "18                                                   Hello!\n",
      "19                                                   I see.\n",
      "20                                                   I try.\n",
      "21                                                   I won!\n",
      "22                                                   I won!\n",
      "23                                                   I won.\n",
      "24                                                   Oh no!\n",
      "25                                                  Attack!\n",
      "26                                                  Attack!\n",
      "27                                                  Cheers!\n",
      "28                                                  Cheers!\n",
      "29                                                  Cheers!\n",
      "                                ...                        \n",
      "167100    A man who has never gone to school may steal f...\n",
      "167101    One way to lower the number of errors in the T...\n",
      "167102    What is old age? First you forget names, then ...\n",
      "167103    What is old age? First you forget names, then ...\n",
      "167104    He and I have a near-telepathic understanding ...\n",
      "167105    Although rainforests make up only two percent ...\n",
      "167106    She has a boyfriend she's been going out with ...\n",
      "167107    If you translate from your second language int...\n",
      "167108    I love trying out new things, so I always buy ...\n",
      "167109    A good theory is characterized by the fact tha...\n",
      "167110    The more time you spend speaking a foreign lan...\n",
      "167111    The enquiry concluded that, despite his denial...\n",
      "167112    The Tatoeba Project, which can be found online...\n",
      "167113    You may not learn to speak as well as a native...\n",
      "167114    And the good news is that today the economy is...\n",
      "167115    E-cigarettes are being promoted as a healthy a...\n",
      "167116    It's still too hard to find a job. And even if...\n",
      "167117    As you contribute more sentences to the Tatoeb...\n",
      "167118    Even at the end of the nineteenth century, sai...\n",
      "167119    Five tremors in excess of magnitude 5.0 on the...\n",
      "167120    No matter how much you try to convince people ...\n",
      "167121    We need to uphold laws against discrimination ...\n",
      "167122    A child who is a native speaker usually knows ...\n",
      "167123    There are four main causes of alcohol-related ...\n",
      "167124    Top-down economics never works, said Obama. \"T...\n",
      "167125    A carbon footprint is the amount of carbon dio...\n",
      "167126    Death is something that we're often discourage...\n",
      "167127    Since there are usually multiple websites on a...\n",
      "167128    If someone who doesn't know your background sa...\n",
      "167129    It may be impossible to get a completely error...\n",
      "Name: en, Length: 167130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(lines.en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lower case\n",
    "import re\n",
    "import string\n",
    "\n",
    "lines.en = lines.en.apply(lambda x : x.lower())\n",
    "lines.de = lines.de.apply(lambda x : x.lower())\n",
    "\n",
    "# process commas\n",
    "lines.en = lines.en.apply(lambda x : re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "lines.de=lines.de.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "\n",
    "# removing punctuations\n",
    "exclude = set(string.punctuation)\n",
    "lines.en = lines.en.apply(lambda x : ''.join(char_ for char_ in x if char_ not in exclude))\n",
    "lines.de = lines.de.apply(lambda x : ''.join(char_ for char_ in x if char_ not in exclude))\n",
    "\n",
    "# getting ride of digits\n",
    "#digits = \"0123456789\"\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "lines.en = lines.en.apply(lambda x: x.translate(remove_digits))\n",
    "lines.de = lines.de.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample processing start from here\n",
    "# appending EOS and SOS to the target data\n",
    "# it creates these symbols for every sentence\n",
    "lines.en = lines.en.apply(lambda x : 'SOS_'+x+'_EOS')\n",
    "#print(lines.en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating word dictionaries\n",
    "en_words = set()\n",
    "for line in lines.en:\n",
    "    for word in line.split():\n",
    "        if word not in en_words:\n",
    "            en_words.add(word)\n",
    "de_words = set()\n",
    "for line in lines.de:\n",
    "    for word in line.split():\n",
    "        if word not in de_words:\n",
    "            de_words.add(word)\n",
    "# get the lengths and sizes\n",
    "# get lengths and sizes :\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in lines.en])+5\n",
    "max_de_words_per_sample = max([len(sample.split()) for sample in lines.de])+5\n",
    "\n",
    "num_en_samples = len(lines.en)\n",
    "num_de_samples = len(lines.de)\n",
    "\n",
    "# Get lists of words :\n",
    "input_words = sorted(list(en_words))\n",
    "target_words = sorted(list(de_words))\n",
    "\n",
    "en_token_to_int = dict()\n",
    "en_int_to_token = dict()\n",
    "\n",
    "de_token_to_int = dict()\n",
    "de_int_to_token = dict()\n",
    "\n",
    "#Tokenizing the words ( Convert them to numbers ) :\n",
    "for i,token in enumerate(input_words):\n",
    "    en_token_to_int[token] = i\n",
    "    en_int_to_token[i]     = token\n",
    "\n",
    "for i,token in enumerate(target_words):\n",
    "    de_token_to_int[token] = i\n",
    "    de_int_to_token[i]     = token\n",
    "\n",
    "# initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "encoder_input_data = np.zeros(\n",
    "    (num_en_samples, max_en_words_per_sample),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_de_samples, max_de_words_per_sample),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_de_samples, max_de_words_per_sample, num_de_words),\n",
    "    dtype='float32')\n",
    "\n",
    "# Process samples, to get input, output, target data:\n",
    "for i, (input_text, target_text) in enumerate(zip(lines.en, lines.de)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = en_token_to_int[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = de_token_to_int[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, de_token_to_int[word]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28946\n"
     ]
    }
   ],
   "source": [
    "print(len(de_token_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28946\n"
     ]
    }
   ],
   "source": [
    "print(len(decoder_target_data[len(encoder_input_data)-1][0]))\n",
    "#167129    It may be impossible to get a completely error...\n",
    "\n",
    "# print(en_int_to_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input_data[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "num_words = len(input_words)\n",
    "vec_len = 300\n",
    "Embedding_layer = keras.layers.Embedding(input_dim = num_words, output_dim = vec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x12cd90fd0>\n"
     ]
    }
   ],
   "source": [
    "print(Embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM,Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Defining some constants: \n",
    "vec_len       = 300   # Length of the vector that we willl get from the embedding layer\n",
    "latent_dim    = 1024  # Hidden layers dimension \n",
    "dropout_rate  = 0.2   # Rate of the dropout layers\n",
    "batch_size    = 64    # Batch size\n",
    "epochs        = 8    # Number of epochs\n",
    "\n",
    "#Input layer of encoder\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "#hidden layers of encoder\n",
    "encoder_embedding = keras.layers.Embedding(input_dim = num_en_words, output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout   = (TimeDistributed(Dropout(rate= dropout_rate)))(encoder_embedding)\n",
    "#encoder_LSTM      = CuDNNLSTM(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "encoder_LSTM      = LSTM(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# output layer of encoder\n",
    "#encoder_LSTM2_layer = CuDNNLSTM(latent_dim, return_state=True)\n",
    "encoder_LSTM2_layer = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder part , encoder states given to the decoder as first state\n",
    "decoder_input = Input(shape = (None,))\n",
    "\n",
    "#Hidden layers of decoder\n",
    "decoder_embedding_layer = keras.layers.Embedding(input_dim = num_de_words, output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "#decoder_LSTM_layer = CuDNNLSTM(latent_dim, return_sequences=True)\n",
    "decoder_LSTM_layer = LSTM(latent_dim, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "#decoder_LSTM_2_layer = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2_layer = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "#output of decoder\n",
    "decoder_dense = Dense(num_de_words, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, None, 300), (None, 1024), (None, 1024)]\n",
      "(?, ?, 300)\n",
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_LSTM_layer.input_shape)\n",
    "print(encoder_embedding.shape)\n",
    "print(encoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 300)    6522900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 300)    0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 300)    8683800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 1024)   5427200     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 300)    0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 1024), (None 8392704     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, None, 1024)   5427200     time_distributed_2[0][0]         \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 1024), 8392704     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 28946)  29669650    lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 72,516,158\n",
      "Trainable params: 72,516,158\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the mode\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "model.summary()    # it prints a summary representation of your model.\n",
    "\n",
    "# Define a checkpoint callback :\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 184 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "184/184 [==============================] - 54s 292ms/step - loss: 0.1703 - val_loss: 0.0565\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05648, saving model to Weights-001--0.05648.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "184/184 [==============================] - 44s 238ms/step - loss: 0.1327 - val_loss: 0.0486\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05648 to 0.04855, saving model to Weights-002--0.04855.hdf5\n",
      "Epoch 3/8\n",
      "184/184 [==============================] - 42s 231ms/step - loss: 0.0973 - val_loss: 0.0461\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04855 to 0.04606, saving model to Weights-003--0.04606.hdf5\n",
      "Epoch 4/8\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.0865 - val_loss: 0.0470\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04606\n",
      "Epoch 5/8\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.0844 - val_loss: 0.0495\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04606\n",
      "Epoch 6/8\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.0827 - val_loss: 0.0496\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04606\n",
      "Epoch 7/8\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.0814 - val_loss: 0.0477\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04606\n",
      "Epoch 8/8\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.0817 - val_loss: 0.0486\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c885470>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_samples = 200\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data[:num_train_samples,:],\n",
    "               decoder_input_data[:num_train_samples,:]],\n",
    "               decoder_target_data[:num_train_samples,:,:],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.08,\n",
    "          callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.callbacks.ModelCheckpoint object at 0x133a1c7f0>]\n"
     ]
    }
   ],
   "source": [
    "print(callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.7891921e-05 3.4521636e-05 3.4533845e-05 ... 3.4530825e-05\n",
      "   3.4542132e-05 3.4541572e-05]]\n",
      "\n",
      " [[4.1902145e-05 3.4522443e-05 3.4492205e-05 ... 3.4528744e-05\n",
      "   3.4550703e-05 3.4518373e-05]]\n",
      "\n",
      " [[4.1902142e-05 3.4522440e-05 3.4492201e-05 ... 3.4528741e-05\n",
      "   3.4550700e-05 3.4518369e-05]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.1902145e-05 3.4522443e-05 3.4492205e-05 ... 3.4528744e-05\n",
      "   3.4550703e-05 3.4518373e-05]]\n",
      "\n",
      " [[4.1902145e-05 3.4522443e-05 3.4492205e-05 ... 3.4528744e-05\n",
      "   3.4550703e-05 3.4518373e-05]]\n",
      "\n",
      " [[4.1902145e-05 3.4522443e-05 3.4492205e-05 ... 3.4528744e-05\n",
      "   3.4550703e-05 3.4518376e-05]]]\n"
     ]
    }
   ],
   "source": [
    "_input = Input(shape=(None,))\n",
    "\n",
    "#hidden layers of encoder\n",
    "_embedding = keras.layers.Embedding(input_dim = num_en_words, output_dim = vec_len)\n",
    "input_ = _embedding(_input)\n",
    "p = model.predict([encoder_input_data[10], decoder_input_data[10]])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 1, 28946)\n"
     ]
    }
   ],
   "source": [
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
